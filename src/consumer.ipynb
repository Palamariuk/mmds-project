{
 "cells": [
  {
   "cell_type": "code",
   "id": "ca85fe4e-3032-4610-b7b8-00dd83894623",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T14:58:29.552874Z",
     "start_time": "2024-11-19T14:58:27.873386Z"
    }
   },
   "source": [
    "!pip install mmh3 bitarray"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mmh3 in /Users/mpalamariuk/anaconda3/envs/mmds-project/lib/python3.9/site-packages (5.0.1)\r\n",
      "Requirement already satisfied: bitarray in /Users/mpalamariuk/anaconda3/envs/mmds-project/lib/python3.9/site-packages (3.0.0)\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "5141fc6715105e0d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T14:58:32.194294Z",
     "start_time": "2024-11-19T14:58:32.184086Z"
    }
   },
   "source": [
    "import pickle\n",
    "\n",
    "import mmh3\n",
    "from bitarray import bitarray\n",
    "\n",
    "\n",
    "class BloomFilter:\n",
    "    def __init__(self, size, hash_count):\n",
    "        self.bit_array = bitarray(size)\n",
    "        self.bit_array.setall(0)\n",
    "        self.size = size\n",
    "        self.hash_count = hash_count\n",
    "\n",
    "    def add(self, item):\n",
    "        for i in range(self.hash_count):\n",
    "            index = mmh3.hash(str(item), i) % self.size\n",
    "            self.bit_array[index] = 1\n",
    "\n",
    "    def __contains__(self, item):\n",
    "        for i in range(self.hash_count):\n",
    "            index = mmh3.hash(str(item), i) % self.size\n",
    "            if self.bit_array[index] == 0:\n",
    "                return False\n",
    "        return True\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "b9447b082bef2a92",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T14:58:34.170095Z",
     "start_time": "2024-11-19T14:58:34.112225Z"
    }
   },
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructField, StructType, StringType, BooleanType, IntegerType\n",
    "from configs import *"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "31ce50c3c027e5e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T14:58:39.207321Z",
     "start_time": "2024-11-19T14:58:34.838773Z"
    }
   },
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"WikiStreamProcessor\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.1\") \\\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/Users/mpalamariuk/anaconda3/envs/mmds-project/lib/python3.9/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/19 16:58:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "ae86b376cd3476d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T14:58:39.951971Z",
     "start_time": "2024-11-19T14:58:39.947163Z"
    }
   },
   "source": [
    "# Define schema for incoming JSON data\n",
    "schema = StructType([\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"timestamp\", StringType(), True),\n",
    "    StructField(\"user\", StringType(), True),\n",
    "    StructField(\"bot\", BooleanType(), True),\n",
    "    StructField(\"minor\", BooleanType(), True),\n",
    "    StructField(\"change\", IntegerType(), True),\n",
    "    StructField(\"comment\", StringType(), True),\n",
    "])"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "e27ae13d69da8ba9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T14:58:41.368226Z",
     "start_time": "2024-11-19T14:58:40.679367Z"
    }
   },
   "source": [
    "with open(\"../artifacts/bloom_state.pkl\", \"rb\") as file:\n",
    "    deserialized_data = pickle.load(file)\n",
    "\n",
    "serialized_data = pickle.dumps(deserialized_data)\n",
    "broadcast_bloom_filter = spark.sparkContext.broadcast(serialized_data)\n",
    "\n",
    "\n",
    "def check_bloom_filter(item):\n",
    "    bit_array, hash_count, size = pickle.loads(broadcast_bloom_filter.value)\n",
    "    for i in range(hash_count):\n",
    "        index = mmh3.hash(str(item), i) % size\n",
    "        if bit_array[index] == 0:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "bloom_udf = F.udf(check_bloom_filter, BooleanType())"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "9b9c58ac-3eff-4d04-8ea2-1816ca024b7b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T14:58:43.629758Z",
     "start_time": "2024-11-19T14:58:41.990281Z"
    }
   },
   "source": [
    "if BROKER_TYPE == 'Socket':\n",
    "    stream_df = (\n",
    "        spark\n",
    "        .readStream\n",
    "        .format(\"socket\")\n",
    "        .option(\"host\", SOCKET_HOST)\n",
    "        .option(\"port\", SOCKET_PORT)\n",
    "        .load()\n",
    "    )\n",
    "else:\n",
    "    stream_df = (\n",
    "        spark\n",
    "        .readStream\n",
    "        .format(\"kafka\")\n",
    "        .option(\"kafka.bootstrap.servers\", KAFKA_BROKER)\n",
    "        .option(\"subscribe\", KAFKA_TOPIC)\n",
    "        .load()\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "tags": [],
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-11-19T14:59:36.057650Z"
    }
   },
   "source": [
    "# Parse JSON from the incoming data stream\n",
    "parsed_df = (\n",
    "    stream_df\n",
    "    .withColumn(\"data\", F.from_json(F.col(\"value\"), schema))\n",
    "    .select(\"data.*\")\n",
    "    .withColumn(\"is_bot_bloom\", bloom_udf(F.col(\"user\")))\n",
    ")\n",
    "\n",
    "metrics_df = parsed_df.withColumn(\n",
    "    \"tp\", F.when((F.col(\"bot\") == 1) & (F.col(\"is_bot_bloom\") == 1), 1).otherwise(0)\n",
    ").withColumn(\n",
    "    \"tn\", F.when((F.col(\"bot\") == 0) & (F.col(\"is_bot_bloom\") == 0), 1).otherwise(0)\n",
    ").withColumn(\n",
    "    \"fp\", F.when((F.col(\"bot\") == 0) & (F.col(\"is_bot_bloom\") == 1), 1).otherwise(0)\n",
    ").withColumn(\n",
    "    \"fn\", F.when((F.col(\"bot\") == 1) & (F.col(\"is_bot_bloom\") == 0), 1).otherwise(0)\n",
    ")\n",
    "\n",
    "# Aggregate the metrics\n",
    "agg_metrics = metrics_df.groupBy().agg(\n",
    "    F.sum(\"tp\").alias(\"tp\"),\n",
    "    F.sum(\"tn\").alias(\"tn\"),\n",
    "    F.sum(\"fp\").alias(\"fp\"),\n",
    "    F.sum(\"fn\").alias(\"fn\")\n",
    ")\n",
    "\n",
    "# Calculate precision, recall, and F1-score\n",
    "final_metrics = agg_metrics.selectExpr(\n",
    "    \"(tp + tn) / (tp + tn + fp + fn) AS accuracy\",\n",
    "    \"tp / (tp + fp) AS precision\",\n",
    "    \"tp / (tp + fn) AS recall\",\n",
    "    \"2 * (tp / (tp + fp) * tp / (tp + fn)) / (tp / (tp + fp) + tp / (tp + fn)) AS f1\",\n",
    "    \"tp\", \"tn\", \"fp\", \"fn\"\n",
    ")\n",
    "\n",
    "# Write the output to console\n",
    "query = final_metrics.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .trigger(processingTime=\"10 second\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+--------+---------+------+----+----+----+----+----+\n",
      "|accuracy|precision|recall|  f1|  tp|  tn|  fp|  fn|\n",
      "+--------+---------+------+----+----+----+----+----+\n",
      "|    null|     null|  null|null|null|null|null|null|\n",
      "+--------+---------+------+----+----+----+----+----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+--------+---------+------+----+---+---+---+---+\n",
      "|accuracy|precision|recall|  f1| tp| tn| fp| fn|\n",
      "+--------+---------+------+----+---+---+---+---+\n",
      "|     1.0|     null|  null|null|  0|  4|  0|  0|\n",
      "+--------+---------+------+----+---+---+---+---+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+------------------+---------+------+----+---+---+---+---+\n",
      "|          accuracy|precision|recall|  f1| tp| tn| fp| fn|\n",
      "+------------------+---------+------+----+---+---+---+---+\n",
      "|0.9285714285714286|      0.0|  null|null|  0| 13|  1|  0|\n",
      "+------------------+---------+------+----+---+---+---+---+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+------------------+---------+------+-------------------+---+---+---+---+\n",
      "|          accuracy|precision|recall|                 f1| tp| tn| fp| fn|\n",
      "+------------------+---------+------+-------------------+---+---+---+---+\n",
      "|0.8709677419354839|      0.2|   1.0|0.33333333333333337|  1| 26|  4|  0|\n",
      "+------------------+---------+------+-------------------+---+---+---+---+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 4\n",
      "-------------------------------------------\n",
      "+------------------+-------------------+------------------+-----------------+---+---+---+---+\n",
      "|          accuracy|          precision|            recall|               f1| tp| tn| fp| fn|\n",
      "+------------------+-------------------+------------------+-----------------+---+---+---+---+\n",
      "|0.8793103448275862|0.45454545454545453|0.8333333333333334|0.588235294117647|  5| 46|  6|  1|\n",
      "+------------------+-------------------+------------------+-----------------+---+---+---+---+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 5\n",
      "-------------------------------------------\n",
      "+------------------+-------------------+------------------+------------------+---+---+---+---+\n",
      "|          accuracy|          precision|            recall|                f1| tp| tn| fp| fn|\n",
      "+------------------+-------------------+------------------+------------------+---+---+---+---+\n",
      "|0.8783783783783784|0.38461538461538464|0.8333333333333334|0.5263157894736842|  5| 60|  8|  1|\n",
      "+------------------+-------------------+------------------+------------------+---+---+---+---+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 6\n",
      "-------------------------------------------\n",
      "+------------------+------------------+------------------+------------------+---+---+---+---+\n",
      "|          accuracy|         precision|            recall|                f1| tp| tn| fp| fn|\n",
      "+------------------+------------------+------------------+------------------+---+---+---+---+\n",
      "|0.8817204301075269|0.4444444444444444|0.8888888888888888|0.5925925925925926|  8| 74| 10|  1|\n",
      "+------------------+------------------+------------------+------------------+---+---+---+---+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 7\n",
      "-------------------------------------------\n",
      "+------------------+------------------+------------------+------------------+---+---+---+---+\n",
      "|          accuracy|         precision|            recall|                f1| tp| tn| fp| fn|\n",
      "+------------------+------------------+------------------+------------------+---+---+---+---+\n",
      "|0.8909090909090909|0.5217391304347826|0.9230769230769231|0.6666666666666666| 12| 86| 11|  1|\n",
      "+------------------+------------------+------------------+------------------+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8bce0aab9faae064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+------------+\n",
      "| id|                text|is_bot_bloom|\n",
      "+---+--------------------+------------+\n",
      "|  1|This is a test se...|       false|\n",
      "|  2|Another example o...|       false|\n",
      "|  3|         ListeriaBot|        true|\n",
      "|  4|Spark is great fo...|       false|\n",
      "+---+--------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define test data\n",
    "data = [\n",
    "    {\"id\": 1, \"text\": \"This is a test sentence.\"},\n",
    "    {\"id\": 2, \"text\": \"Another example of text data.\"},\n",
    "    {\"id\": 3, \"text\": \"ListeriaBot\"},\n",
    "    {\"id\": 4, \"text\": \"Spark is great for distributed computing.\"}\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data)\n",
    "\n",
    "\n",
    "df.withColumn(\"is_bot_bloom\", bloom_udf(F.col(\"text\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf7223b-ffe7-408f-8fd6-709351346280",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
